"""
Experiment Runner

Executes hyperparameter optimization trials and collects metrics.
"""

import sys
import time
from typing import Any, Dict, Optional

import numpy as np
import torch
import torch.nn as nn

from bioplausible.config import GLOBAL_CONFIG
from bioplausible.hyperopt.storage import HyperoptStorage
from bioplausible.hyperopt.tasks import BaseTask, create_task
from bioplausible.models.factory import create_model
from bioplausible.models.registry import ModelSpec, get_model_spec


class TrialRunner:
    """Runs individual hyperparameter optimization trials."""

    def __init__(
        self,
        storage: HyperoptStorage = None,
        device: str = "auto",
        task: str = "shakespeare",
        quick_mode: bool = True,
    ):
        self.storage = storage or HyperoptStorage()
        self.device = (
            "cuda" if (device == "auto" and torch.cuda.is_available()) else device
        )
        self.task_name = task
        self.quick_mode = quick_mode
        self.epochs = GLOBAL_CONFIG.epochs

        # Initialize Task abstraction
        self.task_obj = create_task(task, self.device, quick_mode)
        self.task_obj.setup()

        self.input_dim = self.task_obj.input_dim
        self.output_dim = self.task_obj.output_dim

    def run_trial(self, trial_id: int, pruning_callback=None) -> bool:
        """Run a single trial and record results."""
        # Get trial
        trial = self.storage.get_trial(trial_id)
        if not trial:
            print(f"Trial {trial_id} not found")
            return False

        print(f"\n{'='*60}")
        print(f"Trial {trial_id}: {trial.model_name}")
        print(f"Config: {trial.config}")
        print(f"{'='*60}\n")

        self.storage.update_trial(trial_id, status="running")

        try:
            # Create model using factory directly
            spec = get_model_spec(trial.model_name)
            config = trial.config
            hidden_dim = config.get("hidden_dim", 128)
            num_layers = config.get("num_layers", 4)

            model = create_model(
                spec=spec,
                input_dim=self.input_dim,
                output_dim=self.output_dim,
                hidden_dim=hidden_dim,
                num_layers=num_layers,
                device=self.device,
                task_type=self.task_obj.task_type,
            )

            # Apply hyperparameters
            lr = config.get("lr", spec.default_lr)
            beta = config.get("beta", spec.default_beta) if spec.has_beta else None
            steps = config.get("steps", spec.default_steps) if spec.has_steps else None

            # Additional params (for Hebbian, etc.)
            # If the model or trainer needs these, we should pass them.
            # Trainer takes **kwargs.

            # Create Trainer via Task
            # We pass all config items as kwargs to the trainer
            trainer_kwargs = config.copy()
            # Remove keys that are passed explicitly to avoid conflicts
            for key in ["lr", "steps", "batches_per_epoch", "eval_batches"]:
                if key in trainer_kwargs:
                    del trainer_kwargs[key]

            trainer = self.task_obj.create_trainer(
                model,
                lr=lr,
                steps=steps if steps else 20,
                batches_per_epoch=200 if not GLOBAL_CONFIG.quick_mode else 100,
                eval_batches=50 if not GLOBAL_CONFIG.quick_mode else 20,
                **trainer_kwargs,
            )

            # Manually set beta on model if applicable (since Trainer might not know model internals deeply)
            # Or assume Trainer handles it?
            # SupervisedTrainer does NOT currently set beta.
            # Let's handle it here or update SupervisedTrainer.
            # Base logic in ExperimentAlgorithm was: self.beta = beta.
            # But the model uses config or direct args.
            # LoopedMLP uses config in constructor.
            # If we want to *update* it (e.g. from hyperopt trial that overrides default), we need to set it.
            # But create_model used spec.default_beta.
            # If config has 'beta', create_model didn't see it?
            # Ah, create_model takes 'spec', but 'spec' has defaults.
            # Wait, create_model uses spec.default_beta.
            # We need to update the model if we have a different beta from config.
            # Most BioModels store beta in config object or as attribute.

            if beta is not None and hasattr(model, "config"):
                # BioModel based
                model.config.beta = beta
            if beta is not None and hasattr(model, "beta"):
                model.beta = beta

            # Training Loop
            epoch_times = []

            for epoch in range(self.epochs):
                metrics = trainer.train_epoch()

                # Log
                self.storage.log_epoch(
                    trial_id,
                    epoch,
                    metrics["loss"],
                    metrics.get("accuracy", 0.0),
                    metrics.get("perplexity", 0.0),
                    metrics["time"],
                )

                epoch_times.append(metrics["time"])

                print(
                    f"Epoch {epoch+1}/{self.epochs}: "
                    f"loss={metrics['loss']:.4f}, "
                    f"acc={metrics.get('accuracy', 0.0):.4f}, "
                    f"ppl={metrics.get('perplexity', 0.0):.2f}, "
                    f"time={metrics['time']:.1f}s"
                )

                # Pruning
                if pruning_callback:
                    if pruning_callback(trial_id, epoch + 1, metrics):
                        print(f"✂️ Trial {trial_id} PRUNED at epoch {epoch+1}")
                        self.storage.update_trial(trial_id, status="pruned")
                        return False

            # Final Stats
            avg_iter_time = np.mean(epoch_times) / (
                trainer.batches_per_epoch
                if hasattr(trainer, "batches_per_epoch")
                else 1
            )  # Fallback for RL
            if hasattr(trainer, "episodes_per_epoch"):  # RL
                avg_iter_time = np.mean(epoch_times) / trainer.episodes_per_epoch

            param_count = sum(p.numel() for p in model.parameters())
            param_count_millions = param_count / 1e6

            self.storage.update_trial(
                trial_id,
                status="completed",
                epochs_completed=self.epochs,
                final_loss=metrics["loss"],
                accuracy=metrics.get("accuracy", 0.0),
                perplexity=metrics.get("perplexity", 0.0),
                iteration_time=avg_iter_time,
                param_count=param_count_millions,
            )

            print(f"\n✅ Trial {trial_id} completed successfully!")
            return True

        except Exception as e:
            print(f"\n❌ Trial {trial_id} failed: {e}")
            import traceback

            traceback.print_exc()
            self.storage.update_trial(trial_id, status="failed")
            return False
