"""
Search Space Definitions

Defines the hyperparameter search spaces for each model type in the registry.
"""

from typing import Any, Dict, List, Tuple, Union

import numpy as np

from bioplausible.models.registry import MODEL_REGISTRY

# Type aliases
NumberRange = Tuple[
    float, float, str
]  # (min, max, scale) where scale in ['log', 'linear', 'int']
DiscreteChoice = List[Union[int, float, str]]


class SearchSpace:
    """
    Hyperparameter search space for a model.
    
    Note: This class now only stores parameter definitions.
    All sampling/mutation/crossover is handled by Optuna.
    Use optuna_bridge.create_optuna_space() for optimization.
    """

    def __init__(
        self, name: str, params: Dict[str, Union[NumberRange, DiscreteChoice]]
    ):
        self.name = name
        self.params = params



# Define search spaces for all models
SEARCH_SPACES = {
    "Backprop Baseline": SearchSpace(
        "Backprop Baseline",
        {
            "lr": (1e-5, 1e-2, "log"),
            "hidden_dim": [64, 128, 256, 512],
            "num_layers": [2, 4, 6],
        },
    ),
    "EqProp MLP": SearchSpace(
        "EqProp MLP",
        {
            "lr": (1e-5, 1e-2, "log"),
            "beta": (0.05, 0.5, "linear"),
            "steps": (5, 20, "int"),
            "hidden_dim": [64, 128],
            "num_layers": [5, 10, 15],
        },
    ),
    # Research Models
    "Holomorphic EqProp": SearchSpace(
        "Holomorphic EqProp",
        {
            "lr": (1e-4, 1e-2, "log"),
            "beta": (0.01, 0.3, "linear"),
            "steps": (10, 40, "int"),
            "hidden_dim": [64, 128],
        },
    ),
    "Directed EqProp (Deep EP)": SearchSpace(
        "Directed EqProp (Deep EP)",
        {
            "lr": (1e-4, 1e-2, "log"),
            "beta": (0.1, 0.5, "linear"),
            "steps": (10, 40, "int"),
            "hidden_dim": [64, 128],
        },
    ),
    "Finite-Nudge EqProp": SearchSpace(
        "Finite-Nudge EqProp",
        {
            "lr": (1e-4, 1e-2, "log"),
            "beta": (0.5, 3.0, "linear"),  # Large beta
            "steps": (10, 40, "int"),
            "hidden_dim": [64, 128],
        },
    ),
    "Conv EqProp (CIFAR-10)": SearchSpace(
        "Conv EqProp (CIFAR-10)",
        {
            "lr": (1e-4, 1e-2, "log"),
            "steps": (10, 25, "int"),
            "hidden_dim": [128, 256],
        },
    ),
    # Hybrid & Experimental
    "Adaptive Feedback Alignment": SearchSpace(
        "Adaptive Feedback Alignment",
        {
            "lr": (1e-4, 1e-2, "log"),
            "fa_scale": (0.5, 1.5, "linear"),
            "adapt_rate": (0.001, 0.1, "log"),
            "hidden_dim": [64, 128, 256],
        },
    ),
    "Equilibrium Alignment": SearchSpace(
        "Equilibrium Alignment",
        {
            "lr": (1e-4, 1e-2, "log"),
            "beta": (0.1, 0.5, "linear"),
            "steps": (10, 30, "int"),
            "align_weight": (0.1, 1.0, "linear"),
        },
    ),
    # Add Missing Spaces
    "Layerwise Equilibrium FA": SearchSpace(
        "Layerwise Equilibrium FA",
        {"lr": (1e-4, 1e-2, "log"), "hidden_dim": [64, 128], "num_layers": [2, 4, 6]},
    ),
    "Energy Guided FA": SearchSpace(
        "Energy Guided FA",
        {
            "lr": (1e-4, 1e-2, "log"),
            "energy_scale": (0.1, 1.0, "linear"),
            "hidden_dim": [64, 128],
        },
    ),
    "Predictive Coding Hybrid": SearchSpace(
        "Predictive Coding Hybrid",
        {"lr": (1e-4, 1e-2, "log"), "steps": (10, 30, "int"), "hidden_dim": [64, 128]},
    ),
    "Sparse Equilibrium": SearchSpace(
        "Sparse Equilibrium",
        {
            "lr": (1e-4, 1e-2, "log"),
            "beta": (0.05, 0.3, "linear"),
            "sparsity": (0.1, 0.9, "linear"),
            "hidden_dim": [128, 256],
        },
    ),
    "Momentum Equilibrium": SearchSpace(
        "Momentum Equilibrium",
        {
            "lr": (1e-4, 1e-2, "log"),
            "momentum": (0.5, 0.95, "linear"),
            "steps": (10, 30, "int"),
        },
    ),
    "Stochastic FA": SearchSpace(
        "Stochastic FA",
        {
            "lr": (1e-4, 1e-2, "log"),
            "noise_scale": (0.01, 0.2, "log"),
            "hidden_dim": [64, 128],
        },
    ),
    "Energy Minimizing FA": SearchSpace(
        "Energy Minimizing FA", {"lr": (1e-4, 1e-2, "log"), "hidden_dim": [64, 128]}
    ),
    # Transformers
    "EqProp Transformer (Attention Only)": SearchSpace(
        "EqProp Transformer (Attention Only)",
        {
            "lr": (1e-5, 1e-2, "log"),
            "steps": (5, 12, "int"),
            "hidden_dim": [64, 128, 256],
            "num_layers": [2, 3],
        },
    ),
    "EqProp Transformer (Full)": SearchSpace(
        "EqProp Transformer (Full)",
        {
            "lr": (1e-5, 1e-2, "log"),
            "steps": (5, 20, "int"),
            "hidden_dim": [64, 128],
            "num_layers": [2, 3],
        },
    ),
    "EqProp Transformer (Hybrid)": SearchSpace(
        "EqProp Transformer (Hybrid)",
        {
            "lr": (1e-5, 1e-2, "log"),
            "steps": (5, 15, "int"),
            "hidden_dim": [128, 256],
            "num_layers": [2, 3],
        },
    ),
    "EqProp Transformer (Recurrent)": SearchSpace(
        "EqProp Transformer (Recurrent)",
        {
            "lr": (1e-5, 1e-2, "log"),
            "steps": (10, 30, "int"),
            "hidden_dim": [128, 256],
            "num_layers": [1],  # Recurrent uses single block
        },
    ),
    "DFA (Direct Feedback Alignment)": SearchSpace(
        "DFA (Direct Feedback Alignment)",
        {
            "lr": (1e-5, 1e-2, "log"),
            "hidden_dim": [64, 128, 256],
            "num_layers": [10, 20, 30],
        },
    ),
    "CHL (Contrastive Hebbian)": SearchSpace(
        "CHL (Contrastive Hebbian)",
        {
            "lr": (1e-5, 1e-2, "log"),
            "beta": (0.05, 0.3, "linear"),
            "steps": (10, 30, "int"),
            "hidden_dim": [64, 128, 256],
            "num_layers": [10, 20, 30],
        },
    ),
    "Deep Hebbian (Hundred-Layer)": SearchSpace(
        "Deep Hebbian (Hundred-Layer)",
        {
            "lr": (1e-5, 5e-3, "log"),
            "hidden_dim": [64, 128],
            "num_layers": [50, 100, 150],  # Test deep scaling
        },
    ),
}


def get_search_space(model_name: str) -> SearchSpace:
    """Get the search space for a model."""
    # 1. Try hardcoded spaces first (for customized ranges)
    if model_name in SEARCH_SPACES:
        return SEARCH_SPACES[model_name]

    # 2. Try to generate from registry
    # Check if exact name in registry
    spec = next((s for s in MODEL_REGISTRY if s.name == model_name), None)

    if spec:
        params = {
            "lr": (1e-5, 1e-2, "log"),
            "hidden_dim": [64, 128, 256],
            "num_layers": [2, 4, 6],
        }

        return SearchSpace(model_name, params)

    # 3. Fallback for completely unknown models
    # Try to infer based on name/flags if possible, or raise error
    if "EqProp" in model_name:
        params = {
            "lr": (1e-5, 1e-2, "log"),
            "beta": (0.05, 0.5, "linear"),
            "steps": (5, 20, "int"),
            "hidden_dim": [64, 128],
        }
        return SearchSpace(model_name, params)

    raise ValueError(f"No search space defined for model: {model_name}")
